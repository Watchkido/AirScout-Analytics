import os
import glob
import sys
import re
import warnings
import numpy as np
import pandas as pd
from datetime import datetime
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
from scipy import stats
from config import CONFIG

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=Warning)


def main() -> str | None:
    """
    Pipeline-kompatibler Einstiegspunkt: F√ºhrt csv_info_extractor f√ºr die erste Datei in data/bearbeitet0 aus.
    Gibt den Pfad zur Info-Textdatei zur√ºck oder None bei Fehler.
    """
    csv_ordner = os.path.join(
        os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
        "data", "bearbeitet0"
    )
    csv_files = glob.glob(os.path.join(csv_ordner, "*.csv"))
    if not csv_files:
        print(f"Keine CSV-Datei in {csv_ordner} gefunden!")
        return None
    return csv_info_extractor(csv_files[0])



# üìä CSV ANALYSE UND INFO-EXPORT SCRIPT MIT ERWEITERTEN ANALYTICS
'''
mod_020_csv_analyzer.py
=======================
Dieses Modul bietet ein umfassendes Analyse- und Informations-Export-Skript
f√ºr CSV-Dateien mit Sensordaten, insbesondere f√ºr Projekte im Bereich
AirScout Analytics. Es kombiniert klassische Datenanalyse mit erweiterten
Machine-Learning-Techniken zur Untersuchung von Sensorwerten (z.B.
MQ-Gassensoren).

Hauptfunktionen:
----------------
- **csv_info_extractor**: Robust geladenes Einlesen von CSV-Dateien mit
  automatischer Trennzeichenerkennung, Erzeugung eines ausf√ºhrlichen
  Analyse-Reports als TXT-Datei. Der Report enth√§lt u.a.:
    - Grundlegende Informationen (Shape, Dateigr√∂√üe, Datentypen)
    - Beispielwerte pro Spalte
    - Head/Tail der Daten
    - Nullwert-Analyse mit Beispielen
    - Statistiken f√ºr numerische und kategoriale Spalten
    - Korrelationen und Duplikate
    - Erweiterte Analysen (Clustering, PCA, Zeitreihen, Feature Selection)
    - Zusammenfassung und Empfehlungen
- **erweiterte_sensor_analyse**: F√ºhrt verschiedene ML-Analysen auf
  Sensordaten durch:
    - Clustering von MQ-Sensoren (KMeans)
    - Hauptkomponentenanalyse (PCA)
    - Zeitreihenanalyse (Trends, Variabilit√§t)
    - Auswahl unabh√§ngiger Sensoren (Feature Selection)
- **mq_sensor_clustering**: Gruppiert MQ-Sensoren nach √Ñhnlichkeit ihres
  Verhaltens.
- **hauptkomponenten_analyse**: Reduziert die Dimensionalit√§t der
  Sensordaten und identifiziert die wichtigsten Sensoren pro Komponente.
- **zeitreihen_veraenderungs_analyse**: Analysiert Trends und Variabilit√§t
  der Sensorwerte √ºber die Zeit.
- **unabhaengige_sensoren_waehlen**: Identifiziert und empfiehlt m√∂glichst
  unabh√§ngige Sensoren f√ºr weitere Analysen.

Besonderheiten:
---------------
- Automatische und robuste CSV-Parsing-Strategien (verschiedene
  Trennzeichen, Fehlerbehandlung)
- Speicherung der Analyseberichte an mehreren Zielorten mit Zeitstempel
- Umfangreiche Ausgaben f√ºr Debugging und Nachvollziehbarkeit
- Konfigurierbare Schwellenwerte und Analyseparameter √ºber ein zentrales
  CONFIG-Objekt

Verwendung:
-----------
Das Skript kann direkt ausgef√ºhrt werden und sucht automatisch nach einer
CSV-Datei im Standardordner. Alternativ kann die Hauptfunktion f√ºr beliebige
CSV-Dateien aufgerufen werden.

Beispiel:
---------
    python mod_020_csv_analyzer.py <pfad/zur/datei.csv>

Abh√§ngigkeiten:
---------------
- pandas, numpy, scikit-learn, scipy, matplotlib, seaborn
'''

warnings.filterwarnings('ignore')


def csv_info_extractor(csv_filepath):
    """
    Extrahiert alle wichtigen Informationen aus einer CSV-Datei
    und speichert sie in eine _info.txt-Datei
    """
    try:
        # CSV-Datei robuster laden mit verschiedenen Methoden
        df = None
        
        # Methode 1: Standard CSV-Laden (Komma-separiert)
        try:
            df = pd.read_csv(csv_filepath, sep=',')
            print("‚úÖ CSV mit Komma-Trennung geladen")
        except pd.errors.ParserError:
            print("‚ö†Ô∏è Komma-Parser fehlgeschlagen, versuche alternative Methoden...")
            # Methode 2: Standard ohne explizites Trennzeichen
            try:
                df = pd.read_csv(csv_filepath)
                print("‚úÖ CSV mit Standard-Einstellungen geladen")
            except pd.errors.ParserError:
                # Methode 3: Mit Semikolon als Trennzeichen
                try:
                    df = pd.read_csv(csv_filepath, sep=';')
                    print("‚úÖ CSV mit Semikolon-Trennung geladen")
                except pd.errors.ParserError:
                    # Methode 4: Automatische Trennzeichen-Erkennung
                    try:
                        df = pd.read_csv(csv_filepath, sep=None, engine='python')
                        print("‚úÖ CSV mit automatischer Trennzeichen-Erkennung geladen")
                    except pd.errors.ParserError:
                        # Methode 5: Mit error_bad_lines=False (ignoriert problematische Zeilen)
                        try:
                            df = pd.read_csv(csv_filepath, on_bad_lines='skip')
                            print("‚úÖ CSV geladen (problematische Zeilen √ºbersprungen)")
                        except pd.errors.ParserError:
                            # Methode 6: Als Text einlesen und erste Zeilen analysieren
                            print("üîç Analysiere Datei-Struktur manuell...")
                        with open(csv_filepath, 'r', encoding='utf-8') as f:
                            lines = f.readlines()[:10]  # Erste 10 Zeilen
                        print("üìã Erste 10 Zeilen der Datei:")
                        for i, line in enumerate(lines):
                            print(f"  {i+1:2d}: {line.strip()}")
                        # H√§ufigste Trennzeichen ermitteln
                        separators = [',', ';', '\t', '|', ' ']
                        sep_counts = {}
                        for sep in separators:
                            count = sum(line.count(sep) for line in lines)
                            if count > 0:
                                sep_counts[sep] = count
                        if sep_counts:
                            best_sep = max(sep_counts, key=sep_counts.get)
                            print(f"üéØ Erkanntes Trennzeichen: '{best_sep}' ({sep_counts[best_sep]} Vorkommen)")
                            try:
                                df = pd.read_csv(csv_filepath, sep=best_sep, on_bad_lines='skip')
                                print("‚úÖ CSV mit erkanntem Trennzeichen geladen")
                            except:
                                raise Exception("Alle CSV-Parsing-Methoden fehlgeschlagen")
                        else:
                            raise Exception("Kein g√ºltiges Trennzeichen erkannt")

        # Nach erfolgreichem Laden: DateTime und GPS_DateTime in datetime konvertieren
        if df is not None:
            for spalte in ['DateTime', 'GPS_DateTime']:
                if spalte in df.columns:
                    df[spalte] = pd.to_datetime(df[spalte], errors='coerce')
        
        if df is None:
            raise Exception("CSV-Datei konnte nicht geladen werden")
        
        # Output-Dateiname erstellen (immer .txt-Endung!)
        base_name = os.path.splitext(csv_filepath)[0]
        info_filename = f"{base_name}_info.txt"
        
        # Info-String aufbauen
        info_content = []
        info_content.append("="*80)
        filename = os.path.basename(csv_filepath)
        info_content.append(f"CSV ANALYSE REPORT - {filename}")
        timestamp = datetime.now().strftime('%d.%m.%Y %H:%M:%S')
        info_content.append(f"Erstellt am: {timestamp}")
        info_content.append("="*80)
        
        # 1. GRUNDLEGENDE INFORMATIONEN
        info_content.append("\nüîç 1. GRUNDLEGENDE INFORMATIONEN")
        info_content.append("-" * 40)
        info_content.append(
            f"üìä Shape: {df.shape[0]} Zeilen √ó {df.shape[1]} Spalten"
        )
        file_size_kb = os.path.getsize(csv_filepath) / 1024
        info_content.append(f"üíæ Dateigr√∂√üe: {file_size_kb:.1f} KB")
        info_content.append(f"üî¢ Gesamt-Datenpunkte: {df.size:,}")
        
        # 2. SPALTEN MIT DATENTYPEN
        info_content.append("\nüìã 2. SPALTEN MIT DATENTYPEN")
        info_content.append("-" * 40)
        for i, (col, dtype) in enumerate(df.dtypes.items(), 1):
            # 5 zuf√§llige unterschiedliche Beispielwerte anzeigen
            unique_values = df[col].dropna().unique()
            if len(unique_values) > 0:
                sample_size = min(5, len(unique_values))
                sample_values = np.random.choice(unique_values,
                                                 size=sample_size,
                                                 replace=False)
                sample_str = ", ".join([
                    str(val)[:25] + "..." if len(str(val)) > 25
                    else str(val) for val in sample_values
                ])
                info_content.append(
                    f"{i:2d}. {col:<25} ‚Üí {str(dtype):<12} | "
                    f"Beispiele: {sample_str}"
                )
            else:
                info_content.append(
                    f"{i:2d}. {col:<25} ‚Üí {str(dtype):<12} | "
                    f"Beispiele: (keine Werte)"
                )
        
        # 3. ERSTE 5 ZEILEN (HEAD)
        info_content.append("\nüëÄ 3. ERSTE 5 ZEILEN (HEAD)")
        info_content.append("-" * 40)
        head_str = df.head().to_string(max_cols=None, max_colwidth=25)
        info_content.append(head_str)
        
        # LETZTE 5 ZEILEN (TAIL)
        info_content.append("\nüëÅÔ∏è LETZTE 5 ZEILEN (TAIL)")
        info_content.append("-" * 40)
        tail_str = df.tail().to_string(max_cols=None, max_colwidth=25)
        info_content.append(tail_str)
        
        # 4. INFO() EQUIVALENT
        info_content.append("\nüìä 4. DATAFRAME INFO")
        info_content.append("-" * 40)
        info_content.append(f"RangeIndex: {len(df)} entries, 0 to {len(df)-1}")
        info_content.append(f"Data columns (total {len(df.columns)} columns):")
        
        for i, col in enumerate(df.columns):
            non_null_count = df[col].count()
            dtype = df[col].dtype
            info_content.append(
                f" {i:2d}  {col:<20} {non_null_count:>6} non-null  "
                f"{str(dtype)}"
            )
        
        memory_usage = df.memory_usage(deep=True).sum()
        info_content.append(f"Memory usage: {memory_usage / 1024:.1f} KB")
        
        # 5. NULL VALUES
        info_content.append("\n‚ùå 5. NULL VALUES (FEHLENDE WERTE)")
        info_content.append("-" * 40)
        null_counts = df.isnull().sum()
        total_nulls = null_counts.sum()
        
        if total_nulls == 0:
            info_content.append("‚úÖ Keine fehlenden Werte gefunden!")
        else:
            info_content.append(f"üö® Gesamt fehlende Werte: {total_nulls:,}")
            info_content.append("")
            for col, null_count in null_counts.items():
                if null_count > 0:
                    percentage = (null_count / len(df)) * 100
                    # Zeilennummern mit NULL-Werten finden
                    null_rows = df[df[col].isnull()].index.tolist()
                    
                    # Nur erste 10 Zeilennummern anzeigen, falls zu viele
                    if len(null_rows) <= 10:
                        rows_str = ", ".join(map(str, null_rows))
                    else:
                        first_10 = ", ".join(map(str, null_rows[:10]))
                        rows_str = (f"{first_10}, ... "
                                    f"(+{len(null_rows)-10} weitere)")
                    
                    info_content.append(
                        f"{col:<25}: {null_count:>6} ({percentage:.1f}%) | "
                        f"Zeilen: {rows_str}"
                    )
                    
                    # Zeige 2 Beispielzeilen mit NULL-Werten
                    sample_null_rows = null_rows[:2]  # Erste 2 Zeilen mit NULL
                    if sample_null_rows:
                        info_content.append(
                            f"   üìã Beispiel-Zeilen mit NULL in '{col}':"
                        )
                        for row_idx in sample_null_rows:
                            row_data = df.iloc[row_idx]
                            # Zeige nur die ersten 5 Spalten f√ºr √úbersicht
                            row_preview = []
                            for i, (col_name, value) in enumerate(
                                row_data.items()
                            ):
                                if i >= 5:  # Nur erste 5 Spalten
                                    row_preview.append("...")
                                    break
                                if pd.isna(value):
                                    row_preview.append(f"{col_name}=NULL")
                                else:
                                    val_str = str(value)[:20]
                                    if len(str(value)) > 20:
                                        val_str += "..."
                                    row_preview.append(f"{col_name}={val_str}")
                            
                            info_content.append(
                                f"      Zeile {row_idx}: "
                                f"{' | '.join(row_preview)}"
                            )
                        info_content.append("")  # Leerzeile nach Beispielen
        
        # 6. NUMERISCHE STATISTIKEN
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            info_content.append("\nüî¢ 6. NUMERISCHE STATISTIKEN")
            info_content.append("-" * 40)
            desc = df[numeric_cols].describe()
            desc_str = desc.to_string()
            info_content.append(desc_str)
        
        # 7. KATEGORISCHE SPALTEN ANALYSE
        categorical_cols = df.select_dtypes(include=['object']).columns
        if len(categorical_cols) > 0:
            info_content.append("\nüìù 7. KATEGORISCHE SPALTEN ANALYSE")
            info_content.append("-" * 40)
            for col in categorical_cols:
                unique_count = df[col].nunique()
                mode_values = df[col].mode()
                most_common = (mode_values.iloc[0] if len(mode_values) > 0
                               else "N/A")
                info_content.append(
                    f"{col:<25}: {unique_count:>4} eindeutige Werte, "
                    f"h√§ufigster: '{most_common}'"
                )
        
        # 8. ZUS√ÑTZLICHE WICHTIGE INFORMATIONEN
        info_content.append("\n‚≠ê 8. ZUS√ÑTZLICHE INFORMATIONEN")
        info_content.append("-" * 40)
        
        # Duplikate
        duplicate_count = df.duplicated().sum()
        info_content.append(f"üîÅ Duplikate: {duplicate_count}")
        
        # Komplett leere Zeilen
        empty_rows = df.isnull().all(axis=1).sum()
        info_content.append(f"üì≠ Komplett leere Zeilen: {empty_rows}")
        
        # Spalten mit nur einem Wert
        single_value_cols = [col for col in df.columns if df[col].nunique() <= 1]
        if single_value_cols:
            info_content.append(f"‚ö†Ô∏è  Spalten mit nur einem Wert: {', '.join(single_value_cols)}")
        
        # Korrelationen (nur f√ºr numerische Spalten)
        if len(numeric_cols) > 1:
            info_content.append("\nüîó 9. KORRELATIONEN (NUMERISCHE SPALTEN)")
            info_content.append("-" * 40)
            corr_matrix = df[numeric_cols].corr()
            
            # H√∂chste Korrelationen finden
            high_corr_pairs = []
            korrelationsschwelle = getattr(CONFIG, 'KORRELATIONSSCHWELLE_HOCH', 0.7)
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    corr_val = corr_matrix.iloc[i, j]
                    if abs(corr_val) > korrelationsschwelle:  # Hohe Korrelation (Schwelle aus config)
                        col1, col2 = corr_matrix.columns[i], corr_matrix.columns[j]
                        high_corr_pairs.append(f"{col1} ‚Üî {col2}: {corr_val:.3f}")
            
            if high_corr_pairs:
                info_content.append("üî• Hohe Korrelationen (|r| > 0.7):")
                for pair in high_corr_pairs:
                    info_content.append(f"  {pair}")
            else:
                info_content.append("‚úÖ Keine sehr hohen Korrelationen gefunden")
        
        # 10. ERWEITERTE SENSOR-ANALYSEN (NEU!)
        erweiterte_ergebnisse = {}
        if len(numeric_cols) >= 2:
            info_content.append("\nüî¨ 10. ERWEITERTE SENSOR-ANALYSEN")
            info_content.append("-" * 40)
            
            try:
                base_name = os.path.splitext(csv_filepath)[0]
                erweiterte_ergebnisse = erweiterte_sensor_analyse(df, base_name)
                
                # Cluster-Ergebnisse in Info einbauen
                if 'mq_cluster' in erweiterte_ergebnisse:
                    cluster_info = erweiterte_ergebnisse['mq_cluster']
                    if 'sensor_gruppen' in cluster_info:
                        info_content.append("üéØ MQ-Sensor Clustering:")
                        for gruppe, sensoren in cluster_info['sensor_gruppen'].items():
                            info_content.append(f"   {gruppe}: {', '.join(sensoren)}")
                
                # PCA-Ergebnisse
                if 'pca_analyse' in erweiterte_ergebnisse:
                    pca_info = erweiterte_ergebnisse['pca_analyse'] 
                    if 'erklaerte_varianz' in pca_info:
                        info_content.append("\nüìä Hauptkomponentenanalyse:")
                        for i, varianz in enumerate(pca_info['erklaerte_varianz']):
                            info_content.append(f"   Komponente {i+1}: {varianz:.1%} Varianz")
                
                # Zeitreihen-Ergebnisse
                if 'zeitreihen_analyse' in erweiterte_ergebnisse:
                    zeit_info = erweiterte_ergebnisse['zeitreihen_analyse']
                    if 'variable_sensoren' in zeit_info:
                        info_content.append(f"\n‚è∞ Variable Sensoren: {len(zeit_info['variable_sensoren'])}")
                        for sensor in zeit_info['variable_sensoren'][:3]:
                            info_content.append(f"   {sensor} (hoch variabel)")
                
                # Feature Selection
                if 'feature_selection' in erweiterte_ergebnisse:
                    feature_info = erweiterte_ergebnisse['feature_selection']
                    if 'empfohlene_sensoren' in feature_info:
                        info_content.append(f"\nüéØ Empfohlene unabh√§ngige Sensoren:")
                        for sensor in feature_info['empfohlene_sensoren'][:5]:
                            info_content.append(f"   {sensor}")
                            
            except Exception as e:
                info_content.append(f"‚ö†Ô∏è Erweiterte Analyse fehlgeschlagen: {str(e)}")
                print(f"‚ö†Ô∏è Erweiterte Analyse-Fehler: {e}")

        # 11. ZUSAMMENFASSUNG
        info_content.append("\nüìà 11. ZUSAMMENFASSUNG")
        info_content.append("-" * 40)
        data_quality = ((df.size - total_nulls) / df.size * 100)
        info_content.append(f"‚úÖ Datenqualit√§t: {data_quality:.1f}% vollst√§ndig")
        info_content.append(
            f"üìä Datentypen: {len(numeric_cols)} numerisch, "
            f"{len(categorical_cols)} kategorisch"
        )
        uniqueness = (1 - duplicate_count/len(df)) * 100
        info_content.append(
            f"üîç Einzigartigkeit: {uniqueness:.1f}% eindeutige Zeilen"
        )
        
        # Erweiterte Analyse Zusammenfassung
        if erweiterte_ergebnisse:
            info_content.append(f"üî¨ Erweiterte Analysen: {len(erweiterte_ergebnisse)} Module")
            
            # Empfehlungen basierend auf Analysen
            empfehlungen = []
            if 'mq_cluster' in erweiterte_ergebnisse:
                cluster_anzahl = erweiterte_ergebnisse['mq_cluster'].get('cluster_anzahl', 0)
                empfehlungen.append(f"{cluster_anzahl} MQ-Sensor-Gruppen identifiziert")
            
            if 'feature_selection' in erweiterte_ergebnisse:
                unabhaengige = len(erweiterte_ergebnisse['feature_selection'].get('unabhaengige_sensoren', []))
                empfehlungen.append(f"{unabhaengige} unabh√§ngige Sensoren empfohlen")
            
            if empfehlungen:
                info_content.append("üí° Analyse-Empfehlungen:")
                for emp in empfehlungen:
                    info_content.append(f"   ‚Ä¢ {emp}")
        

        # Speichern nur noch in den beiden Zielorten im ergebnisse-Ordner
        # (Kein Speichern mehr im Originalpfad)

        # 1. Kopie im Ordner ergebnisse mit info_txt_{alter dateiname}
        ergebnisse_ordner = r"E:/dev/projekt_python_venv/airscout-analytics/data/ergebnisse"
        os.makedirs(ergebnisse_ordner, exist_ok=True)
        alt_dateiname = os.path.basename(csv_filepath)
        info_txt_name = f"info_txt_{alt_dateiname}".replace('.csv', '.txt')
        info_txt_path = os.path.join(ergebnisse_ordner, info_txt_name)
        with open(info_txt_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(info_content))

        # 2. Kopie im Unterordner yyyy_mm_dd_hh_mm mit {alter dateiname}_info.txt
        # Extrahiere Zeitstempel aus Dateinamen (Format: yyyy_mm_dd_hh_mm)
        match = re.search(r'(\d{4})_(\d{2})(\d{2})(\d{2})(\d{2})', alt_dateiname)
        if match:
            yyyy_mm_dd_hh_mm = f"{match.group(1)}_{match.group(2)}_{match.group(3)}_{match.group(4)}_{match.group(5)}"
        else:
            yyyy_mm_dd_hh_mm = os.path.splitext(alt_dateiname)[0]
        unterordner = os.path.join(ergebnisse_ordner, yyyy_mm_dd_hh_mm)
        os.makedirs(unterordner, exist_ok=True)
        info_unterordner_name = f"{os.path.splitext(alt_dateiname)[0]}_info.txt"
        info_unterordner_path = os.path.join(unterordner, info_unterordner_name)
        with open(info_unterordner_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(info_content))

        print("‚úÖ Analyse abgeschlossen!")
        print(f"üìÅ Info-Datei erstellt: {info_txt_path}")
        print(f"üìÅ Kopie gespeichert als: {info_unterordner_path}")
        print(f"üìä {len(info_content)} Zeilen Analyse-Information gespeichert")

        # R√ºckgabe: immer .txt-Endung, nicht .csv
        return info_txt_path
        
    except FileNotFoundError:
        print(f"‚ùå Datei nicht gefunden: {csv_filepath}")
        return None
    except pd.errors.EmptyDataError:
        print(f"‚ùå Die Datei ist leer: {csv_filepath}")
        return None
    except pd.errors.ParserError as e:
        print(f"‚ùå Fehler beim Parsen der CSV-Datei: {e}")
        return None
    except Exception as e:
        print(f"‚ùå Fehler beim Analysieren der CSV-Datei: {e}")
        return None


# üöÄ SCRIPT VERWENDUNG:
if __name__ == "__main__":

    print("üöÄ CSV INFO EXTRACTOR")
    print("="*50)

    # Automatische Suche nach erster CSV in data/bearbeitet0
    import glob
    import sys
    csv_ordner = r"E:/dev/projekt_python_venv/airscout-analytics/data/bearbeitet0"
    csv_files = glob.glob(os.path.join(csv_ordner, "*.csv"))
    if csv_files:
        csv_file = csv_files[0]
        print(f"üìÇ Automatisch gefundene Datei: {csv_file}")
    else:
        print(f"‚ùå Keine CSV-Datei in {csv_ordner} gefunden!")
        sys.exit(1)

    # Script ausf√ºhren
    result = csv_info_extractor(csv_file)

    if result:
        print(f"\nüéØ Fertig! Alle Informationen wurden in '{result}' gespeichert.")
        print("\nüí° Das Script kann f√ºr jede CSV-Datei verwendet werden:")
        print("   python csv_analyzer_02.py <pfad/zur/datei.csv>")
    else:
        print("\n‚ùå Analyse konnte nicht abgeschlossen werden.")


def erweiterte_sensor_analyse(df: pd.DataFrame, output_pfad: str) -> dict:
    """
    F√ºhrt erweiterte Analysen f√ºr Sensor-Daten durch.
    
    Diese Funktion implementiert verschiedene Machine Learning Techniken
    zur Analyse von Sensordaten, insbesondere f√ºr MQ-Gassensoren.
    
    :param df: DataFrame mit Sensordaten
    :type df: pd.DataFrame
    :param output_pfad: Pfad f√ºr die Ausgabe-Dateien
    :type output_pfad: str
    :returns: Dictionary mit Analyseergebnissen
    :rtype: dict
    :raises ValueError: Wenn keine numerischen Spalten gefunden werden
    :example:
    
        >>> ergebnisse = erweiterte_sensor_analyse(df, 'C:/output/')
        >>> print(ergebnisse['mq_cluster'])
        
    """
    print("\nüî¨ ERWEITERTE SENSOR-ANALYSE")
    print("=" * 50)
    
    # Numerische Spalten identifizieren
    numerische_spalten = df.select_dtypes(include=[np.number]).columns.tolist()
    if len(numerische_spalten) < 2:
        print("‚ùå Nicht gen√ºgend numerische Spalten f√ºr erweiterte Analyse")
        return {}
    
    # MQ-Sensoren identifizieren (spalten mit 'MQ' oder 'mq' im Namen)
    mq_spalten = [col for col in numerische_spalten 
                  if 'mq' in col.lower() or 'gas' in col.lower()]
    
    ergebnisse = {}
    
    # 1. CLUSTERANALYSE DER MQ-SENSOREN
    if len(mq_spalten) >= 2:
        ergebnisse['mq_cluster'] = mq_sensor_clustering(df, mq_spalten, output_pfad)
    
    # 2. PCA (HAUPTKOMPONENTENANALYSE)
    if len(numerische_spalten) >= 3:
        ergebnisse['pca_analyse'] = hauptkomponenten_analyse(df, numerische_spalten, output_pfad)
    
    # 3. ZEITREIHENANALYSE
    ergebnisse['zeitreihen_analyse'] = zeitreihen_veraenderungs_analyse(df, numerische_spalten, output_pfad)
    
    # 4. FEATURE SELECTION
    if len(numerische_spalten) >= 3:
        ergebnisse['feature_selection'] = unabhaengige_sensoren_waehlen(df, numerische_spalten, output_pfad)
    
    return ergebnisse


def mq_sensor_clustering(df: pd.DataFrame, mq_spalten: list, output_pfad: str) -> dict:
    """
    F√ºhrt Clusteranalyse der MQ-Sensoren durch.
    
    **Methode KMeans()** gruppiert Sensoren basierend auf ihrem Verhalten
    **Argument n_clusters** bestimmt die Anzahl der Gruppen
    **StandardScaler()** normalisiert die Sensor-Werte f√ºr bessere Vergleichbarkeit
    **R√ºckgabewert** ist ein Dictionary mit Cluster-Informationen und Gruppierungen
    
    :param df: DataFrame mit Sensordaten
    :type df: pd.DataFrame  
    :param mq_spalten: Liste der MQ-Sensor Spaltennamen
    :type mq_spalten: list
    :param output_pfad: Pfad f√ºr Ausgabe-Dateien
    :type output_pfad: str
    :returns: Dictionary mit Cluster-Ergebnissen
    :rtype: dict
    """
    print("\nüéØ 1. CLUSTERANALYSE DER MQ-SENSOREN")
    print("-" * 40)
    
    # Daten f√ºr Clustering vorbereiten
    mq_daten = df[mq_spalten].dropna()
    if len(mq_daten) < 10:
        print("‚ö†Ô∏è Zu wenige Datenpunkte f√ºr Clustering")
        return {}
    
    # Daten standardisieren (wichtig f√ºr K-Means!)
    scaler = StandardScaler()
    mq_standardisiert = scaler.fit_transform(mq_daten)
    
    # Optimale Cluster-Anzahl mit Elbow-Methode finden
    max_cluster = min(8, len(mq_spalten))
    inertien = []
    
    for k in range(2, max_cluster + 1):
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(mq_standardisiert)
        inertien.append(kmeans.inertia_)
    
    # Optimale Anzahl w√§hlen (vereinfacht: mittlerer Wert)
    optimale_cluster = len(inertien) // 2 + 2
    
    # Finales Clustering
    kmeans_final = KMeans(n_clusters=optimale_cluster, random_state=42, n_init=10)
    cluster_labels = kmeans_final.fit_predict(mq_standardisiert)
    
    # Ergebnisse zusammenstellen
    cluster_info = {}
    for cluster_id in range(optimale_cluster):
        sensoren_in_cluster = [mq_spalten[i] for i, label in enumerate(cluster_labels) 
                              if label == cluster_id]
        cluster_info[f"Gruppe_{cluster_id + 1}"] = sensoren_in_cluster
    
    print(f"‚úÖ {optimale_cluster} Sensor-Gruppen identifiziert:")
    for gruppe, sensoren in cluster_info.items():
        print(f"   {gruppe}: {', '.join(sensoren)}")
    
    return {
        'cluster_anzahl': optimale_cluster,
        'sensor_gruppen': cluster_info,
        'cluster_zentren': kmeans_final.cluster_centers_
    }


def hauptkomponenten_analyse(df: pd.DataFrame, numerische_spalten: list, output_pfad: str) -> dict:
    """
    F√ºhrt PCA (Hauptkomponentenanalyse) durch zur Dimensionsreduktion.
    
    **Methode PCA()** reduziert die Anzahl der Sensor-Dimensionen auf die wichtigsten
    **Argument n_components** bestimmt, auf wie viele Hauptkomponenten reduziert wird
    **explained_variance_ratio_** zeigt, wie viel Varianz jede Komponente erkl√§rt
    **R√ºckgabewert** ist ein Dictionary mit den Hauptkomponenten und deren Wichtigkeit
    
    :param df: DataFrame mit Sensordaten
    :type df: pd.DataFrame
    :param numerische_spalten: Liste numerischer Spaltennamen  
    :type numerische_spalten: list
    :param output_pfad: Pfad f√ºr Ausgabe-Dateien
    :type output_pfad: str
    :returns: Dictionary mit PCA-Ergebnissen
    :rtype: dict
    """
    print("\nüìä 2. HAUPTKOMPONENTENANALYSE (PCA)")
    print("-" * 40)
    
    # Daten vorbereiten
    sensor_daten = df[numerische_spalten].dropna()
    if len(sensor_daten) < 10:
        print("‚ö†Ô∏è Zu wenige Datenpunkte f√ºr PCA")
        return {}
    
    # Daten standardisieren
    scaler = StandardScaler()
    daten_standardisiert = scaler.fit_transform(sensor_daten)
    
    # PCA auf konfigurierbare Anzahl Komponenten reduzieren
    pca_komponenten_anzahl = getattr(CONFIG, 'PCA_KOMPONENTEN_ANZAHL', 3)
    komponenten_anzahl = min(pca_komponenten_anzahl, len(numerische_spalten))
    pca = PCA(n_components=komponenten_anzahl)
    hauptkomponenten = pca.fit_transform(daten_standardisiert)
    
    # Erkl√§rte Varianz berechnen
    erklaerte_varianz = pca.explained_variance_ratio_
    kumulierte_varianz = np.cumsum(erklaerte_varianz)
    
    print(f"‚úÖ {komponenten_anzahl} Hauptkomponenten extrahiert:")
    for i, (varianz, kumuliert) in enumerate(zip(erklaerte_varianz, kumulierte_varianz)):
        print(f"   Komponente {i+1}: {varianz:.1%} Varianz ({kumuliert:.1%} kumuliert)")
    
    # Wichtigste Sensoren pro Komponente identifizieren
    komponenten_matrix = pca.components_
    wichtige_sensoren = {}
    
    for i in range(komponenten_anzahl):
        # Absolute Werte der Ladungen f√ºr diese Komponente
        ladungen = np.abs(komponenten_matrix[i])
        # Sortiere Sensoren nach Wichtigkeit
        wichtigkeits_indices = np.argsort(ladungen)[::-1]
        top_sensoren = [numerische_spalten[idx] for idx in wichtigkeits_indices[:3]]
        wichtige_sensoren[f"Komponente_{i+1}"] = top_sensoren
        
        print(f"   Top 3 Sensoren Komp. {i+1}: {', '.join(top_sensoren)}")
    
    return {
        'komponenten_anzahl': komponenten_anzahl,
        'erklaerte_varianz': erklaerte_varianz.tolist(),
        'kumulierte_varianz': kumulierte_varianz.tolist(),
        'wichtige_sensoren': wichtige_sensoren,
        'transformierte_daten': hauptkomponenten
    }


def zeitreihen_veraenderungs_analyse(df: pd.DataFrame, numerische_spalten: list, output_pfad: str) -> dict:
    """
    Analysiert zeitliche Ver√§nderungen in Sensor-Daten.
    
    **Methode diff()** berechnet die Differenzen zwischen aufeinanderfolgenden Messungen
    **rolling().mean()** erstellt gleitende Durchschnitte zur Trend-Erkennung
    **std()** misst die Variabilit√§t der Sensor-Werte
    **R√ºckgabewert** ist ein Dictionary mit Trend- und Variabilit√§ts-Informationen
    
    :param df: DataFrame mit Sensordaten
    :type df: pd.DataFrame
    :param numerische_spalten: Liste numerischer Spaltennamen
    :type numerische_spalten: list  
    :param output_pfad: Pfad f√ºr Ausgabe-Dateien
    :type output_pfad: str
    :returns: Dictionary mit Zeitreihen-Analyseergebnissen
    :rtype: dict
    """
    print("\n‚è∞ 3. ZEITREIHENANALYSE")
    print("-" * 40)
    
    if len(df) < 20:
        print("‚ö†Ô∏è Zu wenige Datenpunkte f√ºr Zeitreihenanalyse")
        return {}
    
    # Ver√§nderungsraten berechnen (erste Ableitung)
    veraenderungen = {}
    variabilitaet = {}
    trends = {}
    
    for spalte in numerische_spalten:
        sensor_werte = df[spalte].dropna()
        if len(sensor_werte) < 10:
            continue
            
        # Absolute Ver√§nderungen zwischen aufeinanderfolgenden Messungen
        differenzen = sensor_werte.diff().dropna()
        durchschnittliche_veraenderung = differenzen.mean()
        variabilitaet_sensor = differenzen.std()
        
        # Trend √ºber gleitenden Durchschnitt (letzte 10% der Daten)
        fenster_groesse = max(5, len(sensor_werte) // 10)
        gleitender_durchschnitt = sensor_werte.rolling(window=fenster_groesse).mean()
        trend_richtung = "steigend" if gleitender_durchschnitt.iloc[-1] > gleitender_durchschnitt.iloc[-fenster_groesse] else "fallend"
        
        veraenderungen[spalte] = durchschnittliche_veraenderung
        variabilitaet[spalte] = variabilitaet_sensor
        trends[spalte] = trend_richtung
    
    # Sensoren nach Variabilit√§t sortieren
    sortierte_variabilitaet = sorted(variabilitaet.items(), key=lambda x: x[1], reverse=True)
    
    print("‚úÖ Sensor-Variabilit√§t (h√∂chste zuerst):")
    for sensor, var in sortierte_variabilitaet[:5]:  # Top 5
        trend = trends.get(sensor, "unbekannt")
        print(f"   {sensor}: Variabilit√§t={var:.3f}, Trend={trend}")
    
    # Signifikante Ver√§nderungen identifizieren
    variabilitaetsfaktor = getattr(CONFIG, 'VARIABILITAETSFAKTOR', 1.5)
    signifikante_sensoren = [sensor for sensor, var in variabilitaet.items() 
                           if var > np.mean(list(variabilitaet.values())) * variabilitaetsfaktor]
    
    print(f"\nüî• {len(signifikante_sensoren)} Sensoren mit hoher Variabilit√§t:")
    for sensor in signifikante_sensoren:
        print(f"   {sensor} (m√∂glicherweise umwelt-sensitiv)")
    
    return {
        'veraenderungsraten': veraenderungen,
        'variabilitaet': variabilitaet,
        'trends': trends,
        'variable_sensoren': signifikante_sensoren,
        'stabilste_sensoren': [s for s, v in sorted(variabilitaet.items(), key=lambda x: x[1])[:3]]
    }


def unabhaengige_sensoren_waehlen(df: pd.DataFrame, numerische_spalten: list, output_pfad: str) -> dict:
    """
    W√§hlt Sensoren mit maximaler Unabh√§ngigkeit aus.
    
    **Methode corr()** berechnet Korrelationsmatrix zwischen allen Sensoren
    **SelectKBest** w√§hlt die besten Features basierend auf statistischen Tests
    **mutual_info_regression** misst nichtlineare Abh√§ngigkeiten zwischen Variablen
    **R√ºckgabewert** ist ein Dictionary mit den unabh√§ngigsten Sensoren
    
    :param df: DataFrame mit Sensordaten
    :type df: pd.DataFrame
    :param numerische_spalten: Liste numerischer Spaltennamen
    :type numerische_spalten: list
    :param output_pfad: Pfad f√ºr Ausgabe-Dateien  
    :type output_pfad: str
    :returns: Dictionary mit Feature-Selection Ergebnissen
    :rtype: dict
    """
    print("\nüéØ 4. FEATURE SELECTION (UNABH√ÑNGIGE SENSOREN)")
    print("-" * 40)
    
    sensor_daten = df[numerische_spalten].dropna()
    if len(sensor_daten) < 10:
        print("‚ö†Ô∏è Zu wenige Datenpunkte f√ºr Feature Selection")
        return {}
    
    # Korrelationsmatrix berechnen
    korrelations_matrix = sensor_daten.corr()
    
    # Hoch korrelierte Sensor-Paare finden
    hohe_korrelationen = []
    korrelationsschwelle_sehr_hoch = getattr(CONFIG, 'KORRELATIONSSCHWELLE_SEHR_HOCH', 0.8)
    for i in range(len(korrelations_matrix.columns)):
        for j in range(i+1, len(korrelations_matrix.columns)):
            korr_wert = korrelations_matrix.iloc[i, j]
            if abs(korr_wert) > korrelationsschwelle_sehr_hoch:  # Sehr hohe Korrelation (Schwelle aus config)
                sensor1 = korrelations_matrix.columns[i]
                sensor2 = korrelations_matrix.columns[j]
                hohe_korrelationen.append((sensor1, sensor2, korr_wert))
    
    # Redundante Sensoren identifizieren
    redundante_sensoren = set()
    for sensor1, sensor2, korr in hohe_korrelationen:
        # Sensor mit geringerer Varianz als redundant markieren
        var1 = sensor_daten[sensor1].var()
        var2 = sensor_daten[sensor2].var()
        redundanter_sensor = sensor1 if var1 < var2 else sensor2
        redundante_sensoren.add(redundanter_sensor)

    # Unabh√§ngige Sensoren ausw√§hlen
    unabhaengige_sensoren = [s for s in numerische_spalten if s not in redundante_sensoren]

    # Diversit√§t bewerten (durchschnittliche absolute Korrelation)
    if len(unabhaengige_sensoren) > 1:
        unabhaengige_korr_matrix = sensor_daten[unabhaengige_sensoren].corr()
        durchschnittliche_korrelation = np.abs(unabhaengige_korr_matrix.values).mean()
    else:
        durchschnittliche_korrelation = 0.0

    print(f"‚úÖ {len(hohe_korrelationen)} redundante Sensor-Paare gefunden:")
    for sensor1, sensor2, korr in hohe_korrelationen[:5]:  # Erste 5 zeigen
        print(f"   {sensor1} ‚Üî {sensor2}: r={korr:.3f}")

    print(f"\nüéØ {len(unabhaengige_sensoren)} unabh√§ngige Sensoren ausgew√§hlt:")
    for sensor in unabhaengige_sensoren:
        print(f"   {sensor}")

    print(f"\nüìä Durchschnittliche Korrelation der Auswahl: {durchschnittliche_korrelation:.3f}")

    return {
        'unabhaengige_sensoren': unabhaengige_sensoren,
        'redundante_sensoren': list(redundante_sensoren),
        'hohe_korrelationen': hohe_korrelationen,
        'durchschnittliche_korrelation': durchschnittliche_korrelation,
        'empfohlene_sensoren': unabhaengige_sensoren[:8]  # Top 8 f√ºr praktische Nutzung
    }

